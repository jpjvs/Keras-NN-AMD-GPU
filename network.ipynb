{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "class ModelCNN:\n",
    "\t@staticmethod\n",
    "\tdef build(width, height, depth, classes):\n",
    "\t\t# initialize the model along with the input shape to be\n",
    "\t\t# \"channels last\" and the channels dimension itself\n",
    "\t\tmodel = Sequential()\n",
    "\t\tinputShape = (height, width, depth)\n",
    "\t\tchanDim = -1\n",
    "\n",
    "\t\t# if we are using \"channels first\", update the input shape\n",
    "\t\t# and channels dimension\n",
    "\t\tif K.image_data_format() == \"channels_first\":\n",
    "\t\t\tinputShape = (depth, height, width)\n",
    "\t\t\tchanDim = 1\n",
    "\n",
    "\t\t# first CONV => RELU => CONV => RELU => POOL layer set\n",
    "\t\tmodel.add(Conv2D(32, (3, 3), padding=\"same\",\n",
    "\t\t\tinput_shape=inputShape))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\t\tmodel.add(Dropout(0.25))\n",
    "\n",
    "\t\t# second CONV => RELU => CONV => RELU => POOL layer set\n",
    "\t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\t\tmodel.add(Dropout(0.25))\n",
    "\n",
    "\t\t# first (and only) set of FC => RELU layers\n",
    "\t\tmodel.add(Flatten())\n",
    "\t\tmodel.add(Dense(512))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization())\n",
    "\t\tmodel.add(Dropout(0.5))\n",
    "\n",
    "\t\t# softmax classifier\n",
    "\t\tmodel.add(Dense(classes))\n",
    "\t\tmodel.add(Activation(\"softmax\"))\n",
    "\n",
    "\t\t# return the constructed network architecture\n",
    "\t\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading Fashion MNIST...\n",
      "[INFO] compiling model...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "[INFO] training model...\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 22s 361us/sample - loss: 0.5207 - acc: 0.8250 - val_loss: 0.3050 - val_acc: 0.8893\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 21s 347us/sample - loss: 0.3350 - acc: 0.8800 - val_loss: 0.2720 - val_acc: 0.9010\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 20s 341us/sample - loss: 0.2919 - acc: 0.8954 - val_loss: 0.2556 - val_acc: 0.9086\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 20s 339us/sample - loss: 0.2653 - acc: 0.9035 - val_loss: 0.2522 - val_acc: 0.9104\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 20s 339us/sample - loss: 0.2534 - acc: 0.9079 - val_loss: 0.2272 - val_acc: 0.9166\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 20s 341us/sample - loss: 0.2412 - acc: 0.9118 - val_loss: 0.2224 - val_acc: 0.9176\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 21s 348us/sample - loss: 0.2344 - acc: 0.9143 - val_loss: 0.2163 - val_acc: 0.9219\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 21s 343us/sample - loss: 0.2286 - acc: 0.9158 - val_loss: 0.2189 - val_acc: 0.9180\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 21s 354us/sample - loss: 0.2211 - acc: 0.9198 - val_loss: 0.2144 - val_acc: 0.9194\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 22s 362us/sample - loss: 0.2183 - acc: 0.9206 - val_loss: 0.2064 - val_acc: 0.9250\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 21s 352us/sample - loss: 0.2146 - acc: 0.9204 - val_loss: 0.2093 - val_acc: 0.9239\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 22s 365us/sample - loss: 0.2087 - acc: 0.9236 - val_loss: 0.2179 - val_acc: 0.9202\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 21s 344us/sample - loss: 0.2062 - acc: 0.9246 - val_loss: 0.2109 - val_acc: 0.9214\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 21s 358us/sample - loss: 0.2027 - acc: 0.9264 - val_loss: 0.2017 - val_acc: 0.9271\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 22s 360us/sample - loss: 0.2010 - acc: 0.9270 - val_loss: 0.1976 - val_acc: 0.9267\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 22s 365us/sample - loss: 0.1978 - acc: 0.9287 - val_loss: 0.1968 - val_acc: 0.9274\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 23s 377us/sample - loss: 0.1944 - acc: 0.9297 - val_loss: 0.1958 - val_acc: 0.9275\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 23s 380us/sample - loss: 0.1919 - acc: 0.9296 - val_loss: 0.1984 - val_acc: 0.9262\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 23s 379us/sample - loss: 0.1949 - acc: 0.9296 - val_loss: 0.1981 - val_acc: 0.9276\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 23s 377us/sample - loss: 0.1853 - acc: 0.9321 - val_loss: 0.1903 - val_acc: 0.9283\n"
     ]
    }
   ],
   "source": [
    "# USAGE\n",
    "# python fashion_mnist.py\n",
    "\n",
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "# import the necessary packages\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.python.keras import utils\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "from imutils import build_montages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "# initialize the number of epochs to train for, base learning rate,\n",
    "# and batch size\n",
    "NUM_EPOCHS = 20\n",
    "INIT_LR = 1e-2\n",
    "BS = 32\n",
    "\n",
    "# initialize tensorflow session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "\n",
    "# grab the Fashion MNIST dataset (if this is your first time running\n",
    "# this the dataset will be automatically downloaded)\n",
    "print(\"[INFO] loading Fashion MNIST...\")\n",
    "((trainX, trainY), (testX, testY)) = fashion_mnist.load_data()\n",
    "\n",
    "# if we are using \"channels first\" ordering, then reshape the design\n",
    "# matrix such that the matrix is:\n",
    "# \tnum_samples x depth x rows x columns\n",
    "if K.image_data_format() == \"channels_first\":\n",
    "\ttrainX = trainX.reshape((trainX.shape[0], 1, 28, 28))\n",
    "\ttestX = testX.reshape((testX.shape[0], 1, 28, 28))\n",
    " \n",
    "# otherwise, we are using \"channels last\" ordering, so the design\n",
    "# matrix shape should be: num_samples x rows x columns x depth\n",
    "else:\n",
    "\ttrainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
    "\ttestX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
    " \n",
    "# scale data to the range of [0, 1]\n",
    "trainX = trainX.astype(\"float32\") / 255.0\n",
    "testX = testX.astype(\"float32\") / 255.0\n",
    "\n",
    "# one-hot encode the training and testing labels\n",
    "trainY = utils.to_categorical(trainY, 10)\n",
    "testY = utils.to_categorical(testY, 10)\n",
    "\n",
    "# initialize the label names\n",
    "labelNames = [\"top\", \"trouser\", \"pullover\", \"dress\", \"coat\",\n",
    "\t\"sandal\", \"shirt\", \"sneaker\", \"bag\", \"ankle boot\"]\n",
    "\n",
    "# initialize the optimizer and model\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = SGD(lr=INIT_LR, momentum=0.9, decay=INIT_LR / NUM_EPOCHS)\n",
    "model = ModelCNN.build(width=28, height=28, depth=1, classes=10)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "\n",
    "# train the network\n",
    "print(\"[INFO] training model...\")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "        \n",
    "H = model.fit(trainX, trainY,\n",
    "\tvalidation_data=(testX, testY),\n",
    "\tbatch_size=BS, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-066d96e615e8>:15: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 116 variables.\n",
      "INFO:tensorflow:Converted 116 variables to const ops.\n",
      "\n",
      "Loading the TF graph...\n",
      "Graph Loaded.\n",
      "Collecting all the 'Const' ops from the graph, by running it....\n",
      "Done.\n",
      "Now finding ops in the TF graph that can be dropped for inference\n",
      "Now starting translation to CoreML graph.\n",
      "Automatic shape interpretation succeeded for input blob conv2d_input_1:0\n",
      "1/192: Analysing op name: training_2/SGD/Variable_21 ( type:  Const )\n",
      "2/192: Analysing op name: training_2/SGD/Variable_20 ( type:  Const )\n",
      "3/192: Analysing op name: training_2/SGD/Variable_19 ( type:  Const )\n",
      "4/192: Analysing op name: training_2/SGD/Variable_18 ( type:  Const )\n",
      "5/192: Analysing op name: training_2/SGD/Variable_17 ( type:  Const )\n",
      "6/192: Analysing op name: training_2/SGD/Variable_16 ( type:  Const )\n",
      "7/192: Analysing op name: training_2/SGD/Variable_15 ( type:  Const )\n",
      "8/192: Analysing op name: training_2/SGD/Variable_14 ( type:  Const )\n",
      "9/192: Analysing op name: training_2/SGD/Variable_13 ( type:  Const )\n",
      "10/192: Analysing op name: training_2/SGD/Variable_12 ( type:  Const )\n",
      "11/192: Analysing op name: training_2/SGD/Variable_11 ( type:  Const )\n",
      "12/192: Analysing op name: training_2/SGD/Variable_10 ( type:  Const )\n",
      "13/192: Analysing op name: training_2/SGD/Variable_9 ( type:  Const )\n",
      "14/192: Analysing op name: training_2/SGD/Variable_8 ( type:  Const )\n",
      "15/192: Analysing op name: training_2/SGD/Variable_7 ( type:  Const )\n",
      "16/192: Analysing op name: training_2/SGD/Variable_6 ( type:  Const )\n",
      "17/192: Analysing op name: training_2/SGD/Variable_5 ( type:  Const )\n",
      "18/192: Analysing op name: training_2/SGD/Variable_4 ( type:  Const )\n",
      "19/192: Analysing op name: training_2/SGD/Variable_3 ( type:  Const )\n",
      "20/192: Analysing op name: training_2/SGD/Variable_2 ( type:  Const )\n",
      "21/192: Analysing op name: training_2/SGD/Variable_1 ( type:  Const )\n",
      "22/192: Analysing op name: training_2/SGD/Variable ( type:  Const )\n",
      "23/192: Analysing op name: SGD_1/decay ( type:  Const )\n",
      "24/192: Analysing op name: SGD_1/momentum ( type:  Const )\n",
      "25/192: Analysing op name: SGD_1/lr ( type:  Const )\n",
      "26/192: Analysing op name: SGD_1/iterations ( type:  Const )\n",
      "27/192: Analysing op name: dense_1_1/bias ( type:  Const )\n",
      "28/192: Analysing op name: dense_1_1/BiasAdd/ReadVariableOp ( type:  Identity )\n",
      "29/192: Analysing op name: dense_1_1/kernel ( type:  Const )\n",
      "30/192: Analysing op name: dense_1_1/MatMul/ReadVariableOp ( type:  Identity )\n",
      "31/192: Analysing op name: batch_normalization_v1_4_1/batchnorm/add/y ( type:  Const )\n",
      "32/192: Analysing op name: batch_normalization_v1_4_1/moving_variance ( type:  Const )\n",
      "33/192: Analysing op name: batch_normalization_v1_4_1/batchnorm/ReadVariableOp ( type:  Identity )\n",
      "34/192: Analysing op name: batch_normalization_v1_4_1/batchnorm/add ( type:  Add )\n",
      "35/192: Analysing op name: batch_normalization_v1_4_1/batchnorm/Rsqrt ( type:  Rsqrt )\n",
      "36/192: Analysing op name: batch_normalization_v1_4_1/moving_mean ( type:  Const )\n",
      "37/192: Analysing op name: batch_normalization_v1_4_1/batchnorm/ReadVariableOp_1 ( type:  Identity )\n",
      "38/192: Analysing op name: batch_normalization_v1_4_1/beta ( type:  Const )\n",
      "39/192: Analysing op name: batch_normalization_v1_4_1/batchnorm/ReadVariableOp_2 ( type:  Identity )\n",
      "40/192: Analysing op name: batch_normalization_v1_4_1/gamma ( type:  Const )\n",
      "41/192: Analysing op name: batch_normalization_v1_4_1/batchnorm/mul/ReadVariableOp ( type:  Identity )\n",
      "42/192: Analysing op name: batch_normalization_v1_4_1/batchnorm/mul ( type:  Mul )\n",
      "43/192: Analysing op name: batch_normalization_v1_4_1/batchnorm/mul_2 ( type:  Mul )\n",
      "44/192: Analysing op name: batch_normalization_v1_4_1/batchnorm/sub ( type:  Sub )\n",
      "45/192: Analysing op name: dense_2/bias ( type:  Const )\n",
      "46/192: Analysing op name: dense_2/BiasAdd/ReadVariableOp ( type:  Identity )\n",
      "47/192: Analysing op name: dense_2/kernel ( type:  Const )\n",
      "48/192: Analysing op name: dense_2/MatMul/ReadVariableOp ( type:  Identity )\n",
      "49/192: Analysing op name: flatten_1/Reshape/shape/1 ( type:  Const )\n",
      "50/192: Analysing op name: flatten_1/strided_slice/stack_2 ( type:  Const )\n",
      "51/192: Analysing op name: flatten_1/strided_slice/stack_1 ( type:  Const )\n",
      "52/192: Analysing op name: flatten_1/strided_slice/stack ( type:  Const )\n",
      "53/192: Analysing op name: batch_normalization_v1_3_1/moving_variance ( type:  Const )\n",
      "54/192: Analysing op name: batch_normalization_v1_3_1/FusedBatchNorm/ReadVariableOp_1 ( type:  Identity )\n",
      "55/192: Analysing op name: batch_normalization_v1_3_1/moving_mean ( type:  Const )\n",
      "56/192: Analysing op name: batch_normalization_v1_3_1/FusedBatchNorm/ReadVariableOp ( type:  Identity )\n",
      "57/192: Analysing op name: batch_normalization_v1_3_1/beta ( type:  Const )\n",
      "58/192: Analysing op name: batch_normalization_v1_3_1/ReadVariableOp_1 ( type:  Identity )\n",
      "59/192: Analysing op name: batch_normalization_v1_3_1/gamma ( type:  Const )\n",
      "60/192: Analysing op name: batch_normalization_v1_3_1/ReadVariableOp ( type:  Identity )\n",
      "61/192: Analysing op name: conv2d_3_1/bias ( type:  Const )\n",
      "62/192: Analysing op name: conv2d_3_1/BiasAdd/ReadVariableOp ( type:  Identity )\n",
      "63/192: Analysing op name: conv2d_3_1/kernel ( type:  Const )\n",
      "64/192: Analysing op name: conv2d_3_1/Conv2D/ReadVariableOp ( type:  Identity )\n",
      "65/192: Analysing op name: batch_normalization_v1_2_1/moving_variance ( type:  Const )\n",
      "66/192: Analysing op name: batch_normalization_v1_2_1/FusedBatchNorm/ReadVariableOp_1 ( type:  Identity )\n",
      "67/192: Analysing op name: batch_normalization_v1_2_1/moving_mean ( type:  Const )\n",
      "68/192: Analysing op name: batch_normalization_v1_2_1/FusedBatchNorm/ReadVariableOp ( type:  Identity )\n",
      "69/192: Analysing op name: batch_normalization_v1_2_1/beta ( type:  Const )\n",
      "70/192: Analysing op name: batch_normalization_v1_2_1/ReadVariableOp_1 ( type:  Identity )\n",
      "71/192: Analysing op name: batch_normalization_v1_2_1/gamma ( type:  Const )\n",
      "72/192: Analysing op name: batch_normalization_v1_2_1/ReadVariableOp ( type:  Identity )\n",
      "73/192: Analysing op name: conv2d_2_1/bias ( type:  Const )\n",
      "74/192: Analysing op name: conv2d_2_1/BiasAdd/ReadVariableOp ( type:  Identity )\n",
      "75/192: Analysing op name: conv2d_2_1/kernel ( type:  Const )\n",
      "76/192: Analysing op name: conv2d_2_1/Conv2D/ReadVariableOp ( type:  Identity )\n",
      "77/192: Analysing op name: batch_normalization_v1_1_1/moving_variance ( type:  Const )\n",
      "78/192: Analysing op name: batch_normalization_v1_1_1/FusedBatchNorm/ReadVariableOp_1 ( type:  Identity )\n",
      "79/192: Analysing op name: batch_normalization_v1_1_1/moving_mean ( type:  Const )\n",
      "80/192: Analysing op name: batch_normalization_v1_1_1/FusedBatchNorm/ReadVariableOp ( type:  Identity )\n",
      "81/192: Analysing op name: batch_normalization_v1_1_1/beta ( type:  Const )\n",
      "82/192: Analysing op name: batch_normalization_v1_1_1/ReadVariableOp_1 ( type:  Identity )\n",
      "83/192: Analysing op name: batch_normalization_v1_1_1/gamma ( type:  Const )\n",
      "84/192: Analysing op name: batch_normalization_v1_1_1/ReadVariableOp ( type:  Identity )\n",
      "85/192: Analysing op name: conv2d_1_1/bias ( type:  Const )\n",
      "86/192: Analysing op name: conv2d_1_1/BiasAdd/ReadVariableOp ( type:  Identity )\n",
      "87/192: Analysing op name: conv2d_1_1/kernel ( type:  Const )\n",
      "88/192: Analysing op name: conv2d_1_1/Conv2D/ReadVariableOp ( type:  Identity )\n",
      "89/192: Analysing op name: batch_normalization_v1_5/moving_variance ( type:  Const )\n",
      "90/192: Analysing op name: batch_normalization_v1_5/FusedBatchNorm/ReadVariableOp_1 ( type:  Identity )\n",
      "91/192: Analysing op name: batch_normalization_v1_5/moving_mean ( type:  Const )\n",
      "92/192: Analysing op name: batch_normalization_v1_5/FusedBatchNorm/ReadVariableOp ( type:  Identity )\n",
      "93/192: Analysing op name: batch_normalization_v1_5/beta ( type:  Const )\n",
      "94/192: Analysing op name: batch_normalization_v1_5/ReadVariableOp_1 ( type:  Identity )\n",
      "95/192: Analysing op name: batch_normalization_v1_5/gamma ( type:  Const )\n",
      "96/192: Analysing op name: batch_normalization_v1_5/ReadVariableOp ( type:  Identity )\n",
      "97/192: Analysing op name: conv2d_4/bias ( type:  Const )\n",
      "98/192: Analysing op name: conv2d_4/BiasAdd/ReadVariableOp ( type:  Identity )\n",
      "99/192: Analysing op name: conv2d_4/kernel ( type:  Const )\n",
      "100/192: Analysing op name: conv2d_4/Conv2D/ReadVariableOp ( type:  Identity )\n",
      "101/192: Analysing op name: conv2d_input_1 ( type:  Placeholder )\n",
      "Skipping name of placeholder\n",
      "102/192: Analysing op name: conv2d_4/Conv2D ( type:  Conv2D )\n",
      "103/192: Analysing op name: conv2d_4/BiasAdd ( type:  BiasAdd )\n",
      "104/192: Analysing op name: activation_6/Relu ( type:  Relu )\n",
      "105/192: Analysing op name: batch_normalization_v1_5/FusedBatchNorm ( type:  FusedBatchNorm )\n",
      "106/192: Analysing op name: conv2d_1_1/Conv2D ( type:  Conv2D )\n",
      "107/192: Analysing op name: conv2d_1_1/BiasAdd ( type:  BiasAdd )\n",
      "108/192: Analysing op name: activation_1_1/Relu ( type:  Relu )\n",
      "109/192: Analysing op name: batch_normalization_v1_1_1/FusedBatchNorm ( type:  FusedBatchNorm )\n",
      "110/192: Analysing op name: max_pooling2d_2/MaxPool ( type:  MaxPool )\n",
      "111/192: Analysing op name: dropout_3/Identity ( type:  Identity )\n",
      "112/192: Analysing op name: conv2d_2_1/Conv2D ( type:  Conv2D )\n",
      "113/192: Analysing op name: conv2d_2_1/BiasAdd ( type:  BiasAdd )\n",
      "114/192: Analysing op name: activation_2_1/Relu ( type:  Relu )\n",
      "115/192: Analysing op name: batch_normalization_v1_2_1/FusedBatchNorm ( type:  FusedBatchNorm )\n",
      "116/192: Analysing op name: conv2d_3_1/Conv2D ( type:  Conv2D )\n",
      "117/192: Analysing op name: conv2d_3_1/BiasAdd ( type:  BiasAdd )\n",
      "118/192: Analysing op name: activation_3_1/Relu ( type:  Relu )\n",
      "119/192: Analysing op name: batch_normalization_v1_3_1/FusedBatchNorm ( type:  FusedBatchNorm )\n",
      "120/192: Analysing op name: max_pooling2d_1_1/MaxPool ( type:  MaxPool )\n",
      "121/192: Analysing op name: dropout_1_1/Identity ( type:  Identity )\n",
      "122/192: Analysing op name: flatten_1/Shape ( type:  Shape )\n",
      "123/192: Analysing op name: flatten_1/strided_slice ( type:  StridedSlice )\n",
      "124/192: Analysing op name: flatten_1/Reshape/shape ( type:  Pack )\n",
      "125/192: Analysing op name: flatten_1/Reshape ( type:  Reshape )\n",
      "126/192: Analysing op name: dense_2/MatMul ( type:  MatMul )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/192: Analysing op name: dense_2/BiasAdd ( type:  BiasAdd )\n",
      "128/192: Analysing op name: activation_4_1/Relu ( type:  Relu )\n",
      "129/192: Analysing op name: batch_normalization_v1_4_1/batchnorm/mul_1 ( type:  Mul )\n",
      "130/192: Analysing op name: batch_normalization_v1_4_1/batchnorm/add_1 ( type:  Add )\n",
      "131/192: Analysing op name: dropout_2_1/Identity ( type:  Identity )\n",
      "132/192: Analysing op name: dense_1_1/MatMul ( type:  MatMul )\n",
      "133/192: Analysing op name: dense_1_1/BiasAdd ( type:  BiasAdd )\n",
      "134/192: Analysing op name: activation_5_1/Softmax ( type:  Softmax )\n",
      "135/192: Analysing op name: training/SGD/Variable_21 ( type:  Const )\n",
      "Translation to CoreML spec completed. Now compiling and saving the CoreML model.\n",
      "\n",
      " Core ML model generated. Saved at location: cnn.mlmodel \n",
      "\n",
      "Core ML input(s): \n",
      " [name: \"conv2d_input_1__0\"\n",
      "type {\n",
      "  multiArrayType {\n",
      "    shape: 1\n",
      "    shape: 28\n",
      "    shape: 28\n",
      "    dataType: DOUBLE\n",
      "  }\n",
      "}\n",
      "]\n",
      "Core ML output(s): \n",
      " [name: \"activation_5_1__Softmax__0\"\n",
      "type {\n",
      "  multiArrayType {\n",
      "    shape: 10\n",
      "    dataType: DOUBLE\n",
      "  }\n",
      "}\n",
      "]\n",
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         top       0.89      0.86      0.88      1000\n",
      "     trouser       1.00      0.98      0.99      1000\n",
      "    pullover       0.91      0.90      0.90      1000\n",
      "       dress       0.92      0.93      0.93      1000\n",
      "        coat       0.88      0.89      0.89      1000\n",
      "      sandal       0.99      0.98      0.99      1000\n",
      "       shirt       0.78      0.80      0.79      1000\n",
      "     sneaker       0.96      0.98      0.97      1000\n",
      "         bag       0.99      0.99      0.99      1000\n",
      "  ankle boot       0.98      0.96      0.97      1000\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     10000\n",
      "   macro avg       0.93      0.93      0.93     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "            session, input_graph_def, output_names, freeze_var_names)\n",
    "        return frozen_graph\n",
    "    \n",
    "# def strip_consts(graph_def, max_const_size=32):\n",
    "#     \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "#     strip_def = tf.GraphDef()\n",
    "#     for n0 in graph_def.node:\n",
    "#         n = strip_def.node.add() \n",
    "#         n.MergeFrom(n0)\n",
    "#         if n.op == 'Const':\n",
    "#             tensor = n.attr['value'].tensor\n",
    "#             size = len(tensor.tensor_content)\n",
    "#             if size > max_const_size:\n",
    "#                 tensor.tensor_content = tf.compat.as_bytes(\"<stripped %d bytes>\"%size)\n",
    "#     return strip_def\n",
    "    \n",
    "# def show_graph(graph_def, max_const_size=32):\n",
    "#     \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "#     if hasattr(graph_def, 'as_graph_def'):\n",
    "#         graph_def = graph_def.as_graph_def()\n",
    "#     strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "#     code = \"\"\"\n",
    "#         <script>\n",
    "#           function load() {{\n",
    "#             document.getElementById(\"{id}\").pbtxt = {data};\n",
    "#           }}\n",
    "#         </script>\n",
    "#         <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "#         <div style=\"height:600px\">\n",
    "#           <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "#         </div>\n",
    "#     \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "  \n",
    "#     iframe = \"\"\"\n",
    "#         <iframe seamless style=\"width:800px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "#     \"\"\".format(code.replace('\"', '&quot;'))\n",
    "#     display(HTML(iframe))\n",
    "\n",
    "import tfcoreml\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.backend.set_learning_phase(0) \n",
    "#Save Model to H5 File\n",
    "# model.save('cnn.h5')\n",
    "tf.keras.models.save_model(model, 'cnn.h5')\n",
    "\n",
    "#Load Model from H5 File\n",
    "model = tf.keras.models.load_model('cnn.h5')\n",
    "global graph\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "# sess = tf.Session()\n",
    "# op = sess.graph.get_operations()\n",
    "# [print(m.values()) for m in op][1]\n",
    "\n",
    "#Creating Tensorflow Pb File from Keras Model\n",
    "frozen_graph = freeze_session(K.get_session(), output_names=[out.op.name for out in model.outputs])\n",
    "# show_graph(frozen_graph)\n",
    "tf.train.write_graph(frozen_graph, '/tmp', 'model.pb', as_text=False)\n",
    "\n",
    "# Convert to CoreML\n",
    "tfcoreml.convert(tf_model_path = '/tmp/model.pb',\n",
    "                     mlmodel_path = 'cnn.mlmodel',\n",
    "                     output_feature_names = ['activation_5_1/Softmax:0'])\n",
    "\n",
    "# make predictions on the test set\n",
    "# tf.keras.backend.clear_session()\n",
    "with graph.as_default():\n",
    "    preds = model.predict(testX)\n",
    "\n",
    "# show a nicely formatted classification report\n",
    "print(\"[INFO] evaluating network...\")\n",
    "print(classification_report(testY.argmax(axis=1), preds.argmax(axis=1),\n",
    "\ttarget_names=labelNames))\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "N = NUM_EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
